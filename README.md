# Create Word Embeddings using Word2Vec and GloVe

This repository focuses on creating word embeddings using Word2Vec and GloVe techniques, utilizing the IMDB dataset for experimentation.

## Word Embedding

When dealing with textual data, converting it into numerical format becomes essential before feeding it into any machine learning model. Words can be compared to categorical variables, and one-hot encoding is a common technique used to convert them into numerical representations. However, one-hot encoding words results in a high-dimensional sparse representation, which is not efficient and fails to capture relationships between words.

To address this, word embedding techniques are employed. Two popular methods are highlighted in this repository:

- **Word2Vec**: This technique learns distributed representations of words in a continuous vector space. It captures semantic relationships between words by representing each word as a dense vector.
- **GloVe (Global Vectors for Word Representation)**: GloVe is another method for learning word embeddings. It leverages global word co-occurrence statistics to learn word vectors.

## Learning Objectives

By exploring this repository, you will achieve the following learning objectives:

- Understand and perform text pre-processing to prepare textual data for embedding techniques.
- Train a Word2Vec model and save it in a file for future use.
- Load the saved Word2Vec model to obtain vector representations of words.
- Measure and visualize the similarity between words using the trained Word2Vec model.
- Utilize pre-trained GloVe embeddings to plot the similarity between words and understand their representations.

## Usage

1. Clone the repository:

    ```bash
    git clone https://github.com/Praveen76/Create-Word-Embeddings-using-Word2Vec-and-Glove.git
    ```

2. Navigate to the project directory:

    ```bash
    cd Create-Word-Embeddings-using-Word2Vec-and-Glove
    ```

3. Follow the Jupyter Notebooks provided in the repository to understand and experiment with text pre-processing, Word2Vec, and GloVe embeddings.

## Credits

- This project is created and maintained by [Praveen Kumar](https://github.com/Praveen76).
## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

# Issues:
If you encounter any issues or have suggestions for improvement, please open an issue in the Issues section of this repository.

# Contact:
The code has been tested on Windows system. It should work well on other distributions but has not yet been tested. In case of any issue with installation or otherwise, please contact me on [Linkedin](https://www.linkedin.com/in/praveen-kumar-anwla-49169266/)

Happy coding!!

## **About Me**:
Iâ€™m a seasoned Data Scientist and founder of [TowardsMachineLearning.Org](https://towardsmachinelearning.org/). I've worked on various Machine Learning, NLP, and cutting-edge deep learning frameworks to solve numerous business problems.
